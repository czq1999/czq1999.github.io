<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="预训练模型综述">
<meta property="og:type" content="article">
<meta property="og:title" content="预训练模型综述">
<meta property="og:url" content="http://example.com/2021/07/23/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/index.html">
<meta property="og:site_name" content="陈子强的博客">
<meta property="og:description" content="预训练模型综述">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/07/23/aiBjkIXW3lf8K1y.png">
<meta property="og:image" content="https://i.loli.net/2021/07/23/ZqdyTrxPbOh9WNw.png">
<meta property="og:image" content="https://i.loli.net/2021/07/23/MwUtgmSL5hFv1Ze.png">
<meta property="og:image" content="https://i.loli.net/2021/07/23/iNzF8CySAuTK3ce.png">
<meta property="article:published_time" content="2021-07-23T01:33:05.000Z">
<meta property="article:modified_time" content="2021-07-23T03:07:54.086Z">
<meta property="article:author" content="zqchen">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/07/23/aiBjkIXW3lf8K1y.png">

<link rel="canonical" href="http://example.com/2021/07/23/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>预训练模型综述 | 陈子强的博客</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?eaaa0b4e12e5341798d03199e03e10d8# <app_id>";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">陈子强的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">记录自学和读研的点滴</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">12</span></a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">9</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">9</span></a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>公益 404</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/07/23/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://cdn.jsdelivr.net/gh/czq1999/image-hosting@master/20210506/QQ头像.jpg">
      <meta itemprop="name" content="zqchen">
      <meta itemprop="description" content="在这个内卷的社会，一起加油">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="陈子强的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          预训练模型综述
        </h1>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-07-23 09:33:05 / 修改时间：11:07:54" itemprop="dateCreated datePublished" datetime="2021-07-23T09:33:05+08:00">2021-07-23</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">论文</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87/%E7%BB%BC%E8%BF%B0/" itemprop="url" rel="index"><span itemprop="name">综述</span></a>
                </span>
            </span>

          
            <div class="post-description">预训练模型综述</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="预训练模型综述">预训练模型综述</h1>
<p>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.07139">2106.07139] Pre-Trained Models: Past, Present and Future (arxiv.org)</a></p>
<h2 id="introduction">Introduction</h2>
<p>​ 深度学习CNN，RNN，GNN，attention近些年广泛应用于各种AI任务中。不同于传统的特征工程，深度学习无需手工设计特征，可以从数据中学习到分布式表示作为数据的特征。由于神经网络有着大量的参数，所以深度学习的训练需要大量的数据，以防止过拟合。然后手动标注数据又非常地耗时耗力。</p>
<p>​ 迁移学习是解决这个问题的里程碑，无需在大量的数据上从头训练，在小样本上便可以解决新的问题。迁移学习是一种双阶段学习框架：（1）从一个或多个源任务中捕获知识。（2）在有限的样本下进行微调。</p>
<p>​ 迁移学习缓解了数据饥渴，很快用在CV中。如，在ImageNet上进行预训练。受益于ImageNet中强大的视觉知识分布，使用少量特定任务数据微调这些预先训练的CNN可以在下游任务中表现良好。</p>
<p>​ 不同于CV的监督学习，NLP领域使用自监督学习，例如mask文本，学习文本中的内在相关性来还原mask文本。这种自监督学习的方法无需标记的文本数据便可以学习文本的表示。</p>
<p>​ 早期将深度学习用于NLP时，会出现梯度的消失或爆炸。因此NLP的预训练使用浅层的网络来捕获单词的语义。如，Word2Vec和GloVe。但是，这些词嵌入是静态的，不适用于多语义的单词。这便催生出了预训练的RNN来学习上下文的词嵌入。但是模型的效果会局限于模型的大小和深度。</p>
<p>​ Transformer的提出，使得在NLP领域训练深的模型成为了可能。以Transformer为架构，以语言模型为目标，衍生出了GPT和BERT。巨大的参数可以捕获文本的词汇和语法结构等知识。在下游任务上进行微调便能取得的很好的性能。</p>
<p>​ 对预训练模型进行微调，而不是从头学习模型成为了共识。然而，我们并不清楚大量模型参数的本质，即可解释性较差。不同的PTM往往朝着四个重要方向发展：设计有效的架构、利用丰富的上下文、提高计算效率以及进行解释和理论分析。</p>
<h2 id="background">Background</h2>
<p>​ PTM的发展依赖于迁移学习，经过有监督的预训练发展到现在的自监督预训练。迁移学习旨在从多个源任务中捕获重要知识，然后将这些知识应用到目标任务中。</p>
<p>​ 迁移学习包括两种：特征迁移和参数迁移。特征迁移方法，预训练特征的表示，然后将表示注入到目标任务中。例如word2vec，ELMO。参数迁移遵循一个假设：源任务和目标任务可以共享模型参数或者超参数的先验分布。因此，参数迁移把学习到的知识编码进模型参数中，然后再微调。例如，预训练的CNN，BERT。</p>
<p>​ AlexNet提出后，一系列深度学习便被提出，这些深度神经模型具有更多的参数和更好的拟合能力。将网络的结构变深可以提高模型性能，但是也会带来梯度消失会爆炸，同时模型性能会到达上限，然后随着网络的深度增加而降低。提高在参数初始化进行归一化，和引入残差结构可以有效解决这些问题。</p>
<p><img src="https://i.loli.net/2021/07/23/aiBjkIXW3lf8K1y.png" /></p>
<p>​</p>
<p>​ 迁移学习可以分为四个子类：Inductive Transfer Learning ,Transductive transfer learning, Self-taught Learning, Unsupervised Transfer Learning.</p>
<p>​ 无监督学习是应用无标签的数据进行学习。自监督学习是无监督学习的分支，利用输入数据本身作为监督信息。无监督学习常用于聚类，异常检测等；自监督学习常用于分类和生成。</p>
<h2 id="transformer-and-representative-ptms">Transformer and Representative PTMs</h2>
<h3 id="transformer">Transformer</h3>
<p>​ 在Transformer之前处理文本一直使用RNN，由于RNN具有顺序性，每一时刻按顺序读取单词，并考虑之前单词的隐藏状态，因此很难并行化计算。Transformer是一种自注意力编码器-解码器结构，可以并行化建模输入序列的单词关系。</p>
<p>​ Transformer的编码阶段：对于给定的单词计算与输入序列其他单词注意力分数。这个注意力分数表示其他单词对于生成给定单词表示的贡献程度。最后通过注意力分数计算其他单词的加权平均来表示给定单词。</p>
<p><img src="https://i.loli.net/2021/07/23/ZqdyTrxPbOh9WNw.png" /></p>
<p>如图所示，单词he的表示由所有单词进行加权求和。在解码阶段，从左到右一次只解码一个表示，解码阶段的每一个表示都参考之前的解码结果。</p>
<h3 id="gpt">GPT</h3>
<p>​ 第一个结合Transformer和自监督的模型，在几乎所有的自然语言处理任务上都取得了成功包括，自然语言推理、问答、常识推理、语义相似性。</p>
<p>​ GPT模型是一种自回归模型。优化给定前文条件下，当前词的条件概率。Transformer的decoder结构。<img src="https://i.loli.net/2021/07/23/MwUtgmSL5hFv1Ze.png" /></p>
<p>如图是BERT和GPT的区别，GPT对之前的单词进行自注意力。微调阶段，输入数据得到GPT最后一层的表示，并利用特定任务的标签，以在下游任务上进行微调。</p>
<h3 id="bert">BERT</h3>
<p>​ BERT的出现极大地促进了PTM的发展。采用Bidirectional Encoder Representation Transformer,是一种自编码语言模型。预训练阶段，进行两个任务：MLM+NSP。MLM即随机mask单词，让模型进行还原。BERT可以学习到双向单词表示。NSP预测当前的句子和下一个句子是否为顺应关系。因此可以学习到句子表示。经过预训练，BERT可以获得用于下游任务的鲁棒参数。通过用下游任务的数据修改输入和输出，BERT可以针对任何NLP任务进行微调。BERT可以通过输入一个句子或句子对来有效地处理这些应用。对于输入，它的模式是两个用特殊标记[SEP]连接的句子，可以表示:(1)释义中的句子对，(2)蕴涵中的假设-前提对，(3)问答中的问题-通道对，以及(4)用于文本分类或序列标记的单个句子。对于输出，BERT将为每个标记生成一个标记级表示，可用于处理序列标记或问题回答，特殊标记[CLS]可被送入一个额外的层进行分类。</p>
<h3 id="others">Others</h3>
<p><img src="https://i.loli.net/2021/07/23/iNzF8CySAuTK3ce.png" /></p>
<p>​ RoBERTa：（1）去除NSP任务（2）更多数据,更大batch size,更多训练次数。（3）更长的训练句子。（4）动态改变掩码模式。同时之处NSP任务对于BERT训练没有作用。</p>
<p>​ ALBERT，减少参数。（1）输入单词的嵌入矩阵分解为两个较小的矩阵。（2）强制所有的Transformer层之间共享参数。（3）句序预测代替NSP任务</p>
<h2 id="designing-effective-architectures">Designing Effective Architectures</h2>
<p>统一序列建模+认知启发架构</p>
<h3 id="统一序列建模">统一序列建模</h3>
<p>自然语言下游任务通常分为三类：</p>
<p>（1）自然语言理解：语法分析，句法分析，问答，推理。</p>
<p>（2）开放文本生成：对话生成，故事生成</p>
<p>（3）非开放文本生成：机器翻译，摘要汇总</p>
<p><strong>结合自回归和自编码建模</strong>：</p>
<ul>
<li>PLM
<ul>
<li>XLNet=GPT单向生成+BERT双向理解在预训练中替换token顺序，用自回归预测</li>
<li>MPNet改进了XLNet预训练不知道句子长度，下游任务指导句子长度的问题</li>
</ul></li>
<li>多任务训练
<ul>
<li>UniLM：联合训练不同的语言模型，单向，双向，seq2seq。</li>
<li>GLM：给定变长的mask跨度，模型自己生成mask掉的token，在所有类型任务上达到最优的模型</li>
</ul></li>
</ul>
<p><strong>使用通用的Encoder-Decoder</strong></p>
<p>​ 在GLM之前BERT或GPT都不能解决变长填空问题。BERT的[mask]数量会泄露信息，GPT只能在序列的末尾生成。</p>
<ul>
<li><p>MASS引入 masked-prediction到encoder-decoder架构，但没有解决变长填空问题。</p></li>
<li><p>T5：一个[MASK] 取mask变长的token然后decoder还原</p></li>
<li><p>BART：对源序列进行阶段、删除、替换、打乱、mask</p>
<p>挑战：会带来更多的参数，在自然语言理解的任务上表现不佳</p></li>
</ul>
<h3 id="认知启发架构">认知启发架构</h3>
<p>人类水平的智力远比仅仅理解不同事物之间的联系要复杂得多。在追求人类水平的智能时，理解我们认知功能的宏观结构至关重要，包括决策、逻辑推理、反事实推理和工作记忆。</p>
<p><strong>可维护的工作记忆</strong></p>
<p>​ Transformer的问题是固定的窗口大小，和长度平方的复杂度。这阻碍了在长文档理解中的应用。在实际中，人类没有表现出远程注意力机制，而是不仅记忆和组织同时也会遗忘。类似于LSTM。</p>
<ul>
<li>Transformer-XL：引入片段级别递归和相对位置编码，然后知识隐式地模拟了工作记忆。</li>
<li>CogQA:在多跳阅读中维护一个认知图。由两个系统组成：（1）系统一基于预训练模型。固定窗口大小。（2）系统二基于图神经网络。</li>
<li>CogLTX:利用MemRecall语言模型来选择应该保留在工作记忆中的句子，并利用另一个模型进行回答或分类。</li>
</ul>
<p><strong>可持续的长期记忆</strong></p>
<p>Transformer能够记忆，有研究将Transformer的前馈神经网络替换为key-value记忆网络，发现效果很好，某种程度上说前馈神经网络相当于记忆网络。</p>
<ul>
<li>REALM：为Transformer构建一个可持续的外部记忆，作者逐句张量化整个维基百科，检索相关句子作为上下文进行掩蔽预训练。对于给定数量的训练步骤，张量化的维基百科被异步更新。</li>
<li>RAG：将mask预训练扩展到自回归生成。</li>
</ul>
<h2 id="utilizing-multi-source-data">Utilizing Multi-Source Data</h2>
<p>​ 多语言，多模态，知识增强的PTM。</p>
<h3 id="多语言预训练">多语言预训练</h3>
<p>​ 与训练几个单语言模型相比，用几种语言训练一个模型可以获得更好的性能。</p>
<p>BERT之前有两种方式学习多语言表示：（1）多语言对一起训练多语言LSTM实现多语言翻译。（2）学习语言的不可知约束，如WGAN将语言表示解耦为语言特定和语言无关的表示。多语言任务可以分为理解任务（分类）和生成任务（翻译）</p>
<p>非平行语料库：</p>
<ul>
<li>mBERT:(MMLM )多语言掩码语言建模：很好地学习到跨语言表示</li>
<li>XLM-R:支持100多种语言，更大的预料，带来了更好的性能</li>
</ul>
<p>平行语料库：</p>
<ul>
<li>XLM:利用双语对进行翻译语言建模（TLM）任务，把两个语义匹配的句子合并成一个随机同时mask两个部分，鼓励模型将两个语言的表示对齐在一起。</li>
<li>CLWR：cross-lingual word recovery。 CLWR利用目标语言嵌入通过利用注意机制来表示源语言嵌入，其目标是恢复源语言嵌入。该任务使模型能够学习不同语言之间的单词级对齐。</li>
<li>CLPC：cross-lingual paraphrase classification。CLPC将对齐的句子视为正对，将未对齐的句子样本视为负对，以执行句子级分类，让模型预测输入对是否对齐。可以学习到不同语言之间的句子级别对齐。</li>
<li>ALM：自动从平行句子中生成代码切换序列，并对其执行，这迫使模型仅基于其他语言的上下文进行预测。</li>
<li>InfoXLM:从信息论的角度分析MMLM和TLM，鼓励模型在对比学习的框架下从不对齐的负例中区分出对齐句子。</li>
<li>HICT：扩展了使用对比学习来学习句子级别和单词级别的跨语言表示</li>
<li>ERNIE-M:回译掩码语言模型，并扩展了平行语料库的规模</li>
</ul>
<p>多语言预训练生成模型</p>
<ul>
<li>MASS： 将MLM掩码语言建模扩展到语言生成。随机mask一段tokens然后用自回归的方法进行还原。</li>
<li>mBART :通过添加特殊符号扩展降噪自编码器（DAE）以支持多种语言</li>
<li>XNLG：跨语言自编码器，与DAE不同，XAE的编码输入和解码输出是不同的语言，类似于机器翻译。此外，XNLG以两阶段的方式优化参数。它在第一阶段用MLM和TLM任务训练编码器。然后，它在第二阶段使用DAE和XAE任务修复编码器并训练解码器。通过这种方法对所有参数进行了很好的预训练，填补了MLM预训练和自回归解码微调之间的空白。</li>
</ul>
<h3 id="多模态预训练">多模态预训练</h3>
<p>​ 视频和图像属于vision，文本和语音属于language。难点是将非文本信息融合进BERT</p>
<ul>
<li>ViLBERT:分别处理文本和图像信息，然后传入两个 encoder，然后使用 Transformer 层得到联合 attention 结果
<ul>
<li>首次提出视觉语言预训练</li>
<li>双流输入</li>
<li>三个预训练任务：MLM（掩码语言模型）+SIA（句子图像对齐）+MRC（掩码区域分类）</li>
<li>五个下游任务：VQA(visual question answering)，VCR（visual commonsense expression）,GRE (grouding referring expressions),ITR(image-text retrieval),ZSIR(zero-shot image-text retrieval)</li>
</ul></li>
<li>LXMERT:
<ul>
<li>比ViLBERT多两个预训练任务（MRFR）masked feature regression，VQA</li>
<li>三个下游任务：VQA，graph question answering (GQA) ，natural language for visual reasoning(NLVR2)<br />
</li>
</ul></li>
<li>VisualBERT:极小的代价扩展了BERT，一种简单高效的baseline
<ul>
<li>单流输入</li>
<li>预训练任务：MLM+IA（图文是否匹配）</li>
<li>四个下游任务：VCR+NLVR2+VQA+ITIR</li>
</ul></li>
<li>Uicoder-VL
<ul>
<li>offsite visual detector 移动到端到端版本中，将 Transformer 的图像标记设计为边界框和对象标签特征的总和</li>
<li>三个预训练任务：MLM，SIA（图文是否匹配），MOC（masked object classification ）</li>
<li>三个下游任务：IR，ZSIR，VCR</li>
</ul></li>
<li>VLBERT
<ul>
<li>两个预训练任务：MLM+MOC</li>
<li>发现SIA会降低模型性能</li>
<li>三个下游任务：VQA，VCR， GRE</li>
</ul></li>
<li>B2T2
<ul>
<li>解决VQA</li>
<li>两个预训练任务：MLM+SIA</li>
</ul></li>
<li>VLP
<ul>
<li>解决VQA，Image caption</li>
<li>encoder 和decoder共享参数</li>
<li>预训练任务：bidirectional masked language prediction（BMLP）,sequence to sequence masked language prediction（s2sMLP）</li>
</ul></li>
<li>UITER
<ul>
<li>学习图像和文本之间的统一表征</li>
<li>预训练任务：MLM，SIA，MRC，MRFR</li>
<li>下游任务：VQA，IR，VCR，NLVR2，REC，VE（visual entailment）</li>
</ul></li>
<li>ImageBERT:
<ul>
<li>弱监督收集大规模图文数据</li>
<li>预训练任务：MLM，SIA，MOC，MRFR</li>
<li>下游任务：ITIR</li>
</ul></li>
<li>X-GPT：
<ul>
<li>BERT多模态下理解任务上结果优异，但是不能直接用于生成任务。</li>
<li>预训练任务：image-conditioned masked language modeling (IMLM), image-conditioned denoising autoencoding (IDA), and text-conditioned image feature generation (TIFG)</li>
<li>下游任务：Image-caption</li>
</ul></li>
<li>Oscar：
<ul>
<li>引入图像的目标检测标签作为对齐图像和文本的锚点</li>
<li>动机：图像的目标检测的标签在文本中通常被提及</li>
<li>下游任务：ITIR,IC,novel object captioning(NOC),VQA ,GCQ,NLVR2</li>
</ul></li>
<li>DALLE
<ul>
<li>第一个文本到图像的零样本预训练模型，弥补了文本描述和图像生成之间的差距，尤其是在组合不同物体方面具有出色的能力。</li>
</ul></li>
<li>CogView
<ul>
<li>引入sandwitch transformer和稀疏注意力从而提高准确率和训练稳定性</li>
<li>超越了DDALLE</li>
</ul></li>
<li>CLIP&amp;WENLAN
<ul>
<li>探索为 V&amp;L 预训练扩大网络规模数据并取得巨大成功</li>
<li>大规模分布式预训练</li>
</ul></li>
</ul>
<h3 id="知识增强预训练">知识增强预训练</h3>
<p>​ 引入外部知识例如知识图谱，领域知识，与练数据的额外注释，可以成为统计建模的良好先验。</p>
<ul>
<li><p>结构化知识-知识图谱</p>
<ul>
<li>引入实体-关系 embedding或者他们的对齐</li>
<li>wang2021:基于维基百科实体的描述，整合语言模型损失和知识嵌入损失来获取知识增强表征</li>
<li>一些工作直接将知识图谱的路径甚至是子图与对齐的文本建模，以此保留更多的结构化知识</li>
<li>由于将（实体，关系）与原文本对齐时，在数据预处理会出现噪声。一些工作直接将序列化的知识转变为结构化的文本，让模型自己去学习知识-文本对齐</li>
<li>OAG-BERT：融合多种开放学术图谱中的结构化知识</li>
</ul></li>
<li><p>融合非结构化知识，特定领域或任务的数据</p>
<ul>
<li>继续训练得到领域或任务模型</li>
<li>吸收领域或任务标注数据</li>
</ul></li>
<li><p>可解释融合</p>
<ul>
<li>在下游任务中基于检索方法使用结构化知识</li>
<li>使用适配器在不同的带标注的知识来源上训练，以便区分知识来自哪里</li>
</ul></li>
</ul>
<h2 id="参考">参考</h2>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/381121057">预训练模型最新综述：过去、现在和未来 - 知乎 (zhihu.com)</a></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/NLP/" rel="tag"># NLP</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/06/22/VAE-1/" rel="prev" title="VAE-自编码器">
      <i class="fa fa-chevron-left"></i> VAE-自编码器
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/07/23/%E5%9B%BE%E6%96%87%E9%A2%84%E8%AE%AD%E7%BB%83%E7%BB%BC%E8%BF%B0/" rel="next" title="图文预训练综述">
      图文预训练综述 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0"><span class="nav-number">1.</span> <span class="nav-text">预训练模型综述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#introduction"><span class="nav-number">1.1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#background"><span class="nav-number">1.2.</span> <span class="nav-text">Background</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#transformer-and-representative-ptms"><span class="nav-number">1.3.</span> <span class="nav-text">Transformer and Representative PTMs</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#transformer"><span class="nav-number">1.3.1.</span> <span class="nav-text">Transformer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gpt"><span class="nav-number">1.3.2.</span> <span class="nav-text">GPT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bert"><span class="nav-number">1.3.3.</span> <span class="nav-text">BERT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#others"><span class="nav-number">1.3.4.</span> <span class="nav-text">Others</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#designing-effective-architectures"><span class="nav-number">1.4.</span> <span class="nav-text">Designing Effective Architectures</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%9F%E4%B8%80%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1"><span class="nav-number">1.4.1.</span> <span class="nav-text">统一序列建模</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%A4%E7%9F%A5%E5%90%AF%E5%8F%91%E6%9E%B6%E6%9E%84"><span class="nav-number">1.4.2.</span> <span class="nav-text">认知启发架构</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#utilizing-multi-source-data"><span class="nav-number">1.5.</span> <span class="nav-text">Utilizing Multi-Source Data</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E8%AF%AD%E8%A8%80%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="nav-number">1.5.1.</span> <span class="nav-text">多语言预训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="nav-number">1.5.2.</span> <span class="nav-text">多模态预训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="nav-number">1.5.3.</span> <span class="nav-text">知识增强预训练</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">1.6.</span> <span class="nav-text">参考</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="zqchen"
      src="https://cdn.jsdelivr.net/gh/czq1999/image-hosting@master/20210506/QQ头像.jpg">
  <p class="site-author-name" itemprop="name">zqchen</p>
  <div class="site-description" itemprop="description">在这个内卷的社会，一起加油</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/czq1999/" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;czq1999&#x2F;" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://si.ahu.edu.cn/" title="http:&#x2F;&#x2F;si.ahu.edu.cn&#x2F;" rel="noopener" target="_blank">安徽大学互联网学院</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://cic.tju.edu.cn/" title="http:&#x2F;&#x2F;cic.tju.edu.cn&#x2F;" rel="noopener" target="_blank">天津大学智能与计算学部</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2021 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">zqchen</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/three-waves.min.js"></script>


  















  

  

  

</body>
</html>
